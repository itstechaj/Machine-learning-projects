{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boston House Price Predictor using ML\n",
        "\n",
        "Project Information:\n",
        "This is my 1st Machine learning Project. I am taking data of boston house pricing from kaggle and apllying machine learning\n",
        "to predict prices if we have provided with new feature of a house.\n",
        "\n",
        "Date - 10/07/2022\n",
        "\n",
        "Time - 2:56 AM\n",
        "\n",
        "Author - Abhijit Kumar(Nit Durgapur CSE)\n",
        "\n",
        "##Atrributes of the data\n",
        "\n",
        "1. CRIM: per capita crime rate by town\n",
        "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS: proportion of non-retail business acres per town\n",
        "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "5. NOX: nitric oxides concentration (parts per 10 million)\n",
        "6. RM: average number of rooms per dwelling\n",
        "7. AGE: proportion of owner-occupied units built prior to 1940\n",
        "8. DIS: weighted distances to five Boston employment centres\n",
        "9. RAD: index of accessibility to radial highways\n",
        "10. TAX: full-value property-tax rate per $10,000\n",
        "11. PTRATIO: pupil-teacher ratio by town\n",
        "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT: % lower status of the population\n",
        "14. MEDV: Median value of owner-occupied homes in"
      ],
      "metadata": {
        "id": "tK9JsC7gBUQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split #used for spliting dataset into training and testing\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#import sklearn.preprocessing, sklearn.decomposition, \\sklearn.linear_model, sklearn.pipeline, sklearn.metrics, \\sklearn.compose\n",
        "# import sklearn_pandas\n"
      ],
      "metadata": {
        "id": "56u7st_W1t0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
        "#reading data in csv file and creating a dataframe\n",
        "csv_df=pd.read_csv(\"/Boston_house_price_data.csv\",names=feature_names)\n",
        "csv_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L-0cTllc4eay",
        "outputId": "0191816e-4456-4669-acc9-bc99f79f3153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
              "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
              "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
              "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
              "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
              "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
              "\n",
              "        B  LSTAT  MEDV  \n",
              "0  396.90   4.98  24.0  \n",
              "1  396.90   9.14  21.6  \n",
              "2  392.83   4.03  34.7  \n",
              "3  394.63   2.94  33.4  \n",
              "4  396.90   5.33  36.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e98b4055-725e-4664-98a2-905c4377e238\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e98b4055-725e-4664-98a2-905c4377e238')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e98b4055-725e-4664-98a2-905c4377e238 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e98b4055-725e-4664-98a2-905c4377e238');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analyzing the Data"
      ],
      "metadata": {
        "id": "6rZZmOvBFYJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps\n",
        "# 1) csv_df.info()\n",
        "# 2) csv_df['column_name'].value_counts()\n",
        "# 3) csv_df.describe()\n",
        "# 4) csv_df.hist(bins=50,figsize=(20,10))\n",
        "## Here we also seperate our data into features and labels to simplify the process of splitting\n",
        "labels_data=csv_df['MEDV'].copy(deep=True)\n",
        "features_data=csv_df.drop('MEDV',axis=1).copy(deep=True)"
      ],
      "metadata": {
        "id": "M8kMK6JQFqdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting Training and testing data\n",
        "At this point we should  split our data into training and testing so that we will not conclude unnecessary patterns from all data.\n",
        "\n",
        "*Proablem to be taken care of:-*\n",
        "\n",
        "But we have to take take of uniformness of data while spliting Eg:- if a feature has binary values and 100 times it exists as 1 and 20 times it exists as 0 .So there might be a case that we accidently split the data in such a way that all ones came in training so our model will make wrong assumtion (our model think that there is only one value possible for this feature.)\n",
        "\n",
        "Solution ‚û°\n",
        "\n",
        "We use stratified sampling.for that we use inbuilt class of sklearn that is train_test_split(We can also use StratifiedShuffleSplit\n",
        "`(sklearn.model_selection.train_test_split)`\n",
        "\n",
        "##inside this train_test_split method there is option for stratified spliting.\n",
        "`sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)`"
      ],
      "metadata": {
        "id": "z6vZpPI4KfOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we done stratified sampling on the basis of 'CHAS' feature\n",
        "features,features_test,labels,labels_test=train_test_split(features_data,labels_data,test_size=0.2,train_size=0.8,random_state=42,shuffle=True,stratify=features_data['CHAS'])\n",
        "X_test=features_test.to_numpy()\n",
        "Y_test=labels_test.to_numpy()\n",
        "#From here on we do not touch our test datas we only work on our trianing data"
      ],
      "metadata": {
        "id": "Mi6e3_gfJp9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Correlation\n",
        "Now we try to find or explore the correlation of every feature with label for that we use df.corrwith() function .it output float value between -1 to 1 where -1 -> very strong negetive relation and +1 -> very strong positve relation.\n",
        "We can also use seaborn heatmap plotting or scatter_matrix plotting using pandas.plotting\n",
        "\n"
      ],
      "metadata": {
        "id": "I9cREJ2ZO8K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We also created a fulldata_train by adding features and labels for future operations\n",
        "fulldata_train=features.join(labels).copy(deep=True)\n",
        "#finding and plottinf correlation matrix\n",
        "corr_matrix=fulldata_train.corr()\n",
        "#heatmap plotting using seaborn\n",
        "'''\n",
        "import seaborn as sns\n",
        "sns.heatmap(corr_matrix.loc[:,'MEDV':])\n",
        "'''\n",
        "#scatter_matrix plotting using pandas.plotting\n",
        "'''\n",
        "from pandas.plotting import scatter_matrix\n",
        "#its better that you use attributes and see plots and then change items in it and again see plots its better otherwise we can see heatmap\n",
        "attributes=['CRIM','RM','AGE','MEDV']\n",
        "scatter_matrix(fulldata_train[attributes],alpha=0.8,figsize=(12,10))\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "R42iHWw1M5eA",
        "outputId": "0d9f426f-6642-42b9-ef01-9be11770e368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom pandas.plotting import scatter_matrix\\n#its better that you use attributes and see plots and then change items in it and again see plots its better otherwise we can see heatmap\\nattributes=['CRIM','RM','AGE','MEDV']\\nscatter_matrix(fulldata_train[attributes],alpha=0.8,figsize=(12,10))\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Taking care of missing attributes\n",
        "you have three option to deal with it\n",
        "\n",
        "1) Get rid of the missing data points\n",
        "\n",
        "`df.dropna(subset=[\"attribute_name\"])` #it does not change original df so for changing in original df we have to use inplace=True\n",
        "\n",
        "2)Get rid of whole attribute\n",
        "\n",
        "`df.drop(\"attribute_name\",axis=1,inplace=True)`\n",
        "\n",
        "3)reaplace NA value with some other value (that do not chaange in your model results)\n",
        "\n",
        "(we are replacing here with median)\n",
        "\n",
        "`median=df[\"atrribute_name\"].median()`\n",
        "\n",
        "`df[\"attribute_name\"].fillna(median)`\n",
        "\n",
        "##But for doing this task we already have an inbuilt method (Imputer)\n",
        "`from sklearn.impute import SimpleImputer`\n",
        "\n",
        "`imputer=SimpleImputer(strategy=\"median\")`\n",
        "\n",
        "`imputer.fit(df)`\n",
        "\n",
        "`imputed_array=imputer.transform(df)` #this is an ndarray so we can convert it into pandas dataframe\n",
        "\n",
        "`tranformed_df=pd.DataFrame(imputed_array,columns=df.columns)` #tranformed_df stores df with all NA replaced by their respective medians`\n",
        "\n",
        "we can also do above steps by making a pipeline (recommended).\n"
      ],
      "metadata": {
        "id": "fP2DJWysZVG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# we can do this in pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(strategy=\"median\")\n",
        "imputer.fit(fulldata_train)\n",
        "imputed_array=imputer.transform(fulldata_train)\n",
        "tranformed_df=pd.DataFrame(imputed_array,columns=fulldata_train.columns)\n",
        "'''"
      ],
      "metadata": {
        "id": "2axqqSm-t6m5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "97f2f528-9bbf-4fe2-c286-fc1b6cd3deba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# we can do this in pipeline\\nfrom sklearn.impute import SimpleImputer\\nimputer=SimpleImputer(strategy=\"median\")\\nimputer.fit(fulldata_train)\\nimputed_array=imputer.transform(fulldata_train)\\ntranformed_df=pd.DataFrame(imputed_array,columns=fulldata_train.columns)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scikit-learn Design\n",
        "Primary three type of objects:-\n",
        "\n",
        "**1) Estimators**\n",
        "\n",
        "It estimates some parameters based on the datasets ex:- Imputer\n",
        "\n",
        "It has a fit method-> fits the dataset and calculate internal parameters and tranform method.\n",
        "\n",
        "**2) Tranformers**\n",
        "\n",
        "transform method takes input and return output based on the learning from fit() of estimiator.\n",
        "\n",
        "It also as a convineance function called `fit_transform()` {faster} which fits and then transform simultaneously.\n",
        "\n",
        "**3) Predictors**\n",
        "\n",
        "LinearRegression model is an Exaple of predictor.It has two common functions a)`fit()` -> fits the training data and create model b)`predict()`-> predicts the output based on the learning of the fit.\n",
        "\n",
        "it also gives the `score()` function which will evaluate the predictions."
      ],
      "metadata": {
        "id": "FrK3CeOBcy5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features-scaling\n",
        "**Why we need this concept?**\n",
        "\n",
        "Value of different features can be very diffrent ex:-faetureA values can range in [0,1] and value of featureB can range in [-1000,1000] so for better modelling we somehow try that if we can scale all feature in same range that's why we use feature scaling.\n",
        "\n",
        "**Primary there are two types of feature scaling.**\n",
        "\n",
        "1) Min-Max Scaling (Normalisation)\n",
        "\n",
        "Ex:- let there be a featureA which minmum value is minm and maximum value is maxm.so for each value val of featureA  we do\n",
        "\n",
        "scaled_val=(val-min)/(max-min) (#range of scaled_val is [0,1])\n",
        "\n",
        "sklearn provides a class called `MinMaxScaler` for minmax scaling.\n",
        "\n",
        "2) Standardization\n",
        "\n",
        "val->values of feature, mean-> mean of all values of feature , std-> standard deviation of feature.\n",
        "\n",
        "scaled_value=(val-mean)/(std)\n",
        "\n",
        "sklearn provides a class called `StandardScaler` for this scaling.\n",
        "\n"
      ],
      "metadata": {
        "id": "8NZeyeT7epFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# we can do this in pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standard_scaler=StandardScaler()\n",
        "features=pd.DataFrame(standard_scaler.fit_transform(features),column=features.columns)\n",
        "features.head()\n",
        "'''"
      ],
      "metadata": {
        "id": "o0a7njZBelzP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "57adfced-7053-4438-9cd7-44ded2fd4549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# we can do this in pipeline\\n\\nfrom sklearn.preprocessing import StandardScaler\\nstandard_scaler=StandardScaler()\\nfeatures=pd.DataFrame(standard_scaler.fit_transform(features),column=features.columns)\\nfeatures.head()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a Pipeline\n",
        "In a pipeline we add diffrent processes.a pipeline takes the data and run the process on it and pass the output to the next process to execute.\n",
        "\n",
        "`from sklearn.pipeline import Pipeline`\n",
        "\n",
        "`from sklearn.preprocessing import StandardScaler`\n",
        "\n",
        "`from sklearn.model_selection import SimpleImputer`\n",
        "\n",
        "`my_pipeline=Pipeline([`\n",
        "\n",
        "  `('imputer',SimpleImputer(strategy=\"median\")),#We can add as many process as we want`\n",
        "\n",
        "  `('std_scaler',StandardScaler())`\n",
        "\n",
        "`])`\n",
        "\n",
        "After that we can directly use fit_tranform method of pipeline.and this fit transform return a numpy array."
      ],
      "metadata": {
        "id": "BNCSPXB9v4h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will crete a pipeline here and add Imputer and scaling into pipeline\n",
        "my_pipeline=Pipeline([\n",
        "('imputer',SimpleImputer(strategy=\"median\")),#We can add as many process as we want\n",
        "('std_scaler',StandardScaler())\n",
        "])\n",
        "'''\n",
        "# Parllel plotting of dataframe\n",
        "fulldata_train=pd.DataFrame(my_pipeline.fit_transform(fulldata_train),columns=fulldata_train.columns)\n",
        "ax=pd.plotting.parallel_coordinates(fulldata_train,'MEDV')\n",
        "legend =ax.legend()\n",
        "legend.remove()\n",
        "plt.show()\n",
        "'''\n",
        "# fit and transform the pipeline on features\n",
        "#we are just giving a new name to the features as X_train and labels to Y_train\n",
        "X_train=my_pipeline.fit_transform(features)\n",
        "Y_train=labels.to_numpy()\n",
        "\n",
        "# this below step is very important because if our training feature have passed through pipeline then our testing features should also\n",
        "# pass through pipeline\n",
        "X_test=my_pipeline.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "BNwQo_kryS4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select your desired model for modelling\n",
        "Example models for Regression\n",
        "\n",
        "**DecisionTreeRegressor**\n",
        "\n",
        "syntax\n",
        "\n",
        "`from sklearn.tree import DecisionTreeRegressor`\n",
        "\n",
        "**Gradient Boosting Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.ensemble import GradientBoostingRegressor`\n",
        "\n",
        "\n",
        "**Elastic Net Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.linear_model import ElasticNe`t\n",
        "\n",
        "**Stochastic Gradient Descent Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.linear_model import SGDRegressor`\n",
        "\n",
        "**Support Vector Machine**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.svm import SVR`\n",
        "\n",
        "**Bayesian Ridge Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.linear_model import BayesianRidge`\n",
        "\n",
        "**CatBoost Regressor**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from catboost import CatBoostRegressor`\n",
        "\n",
        "**Kernel Ridge Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.kernel_ridge import KernelRidge`\n",
        "\n",
        "**Linear Regression**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.linear_model import LinearRegression`\n",
        "\n",
        "**KNeighbors Regressor**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.neighbors import KNeighborsRegressor`\n",
        "\n",
        "**RandomForestRegressor**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.ensemble import RandomForestRegressor`\n",
        "\n",
        "**XGBoost Regressor**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from xgboost.sklearn import XGBRegressor`\n",
        "\n",
        "**LGBM Regressor**\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from lightgbm import LGBMRegressor`\n",
        "\n",
        "**You can allways check for k-fold cross validation (to check if our model get overfitted on training dataset)**\n",
        "\n",
        "K-fold cross validation makes  k groups randomly in our dataset and run our model on it and calculate the error for every fold and return it.\n",
        "\n",
        "Syntax\n",
        "\n",
        "`from sklearn.model_selection import cross_val_score`\n",
        "\n",
        "`cross_val_score(model/estimator,x,y,scoring=\"neg_mean_squared_error\",cv=10(->groups of folding))`\n",
        "\n",
        "it return neg of mean sqaured error so we have to make it positive and then take its root to get root mean squred  error.\n"
      ],
      "metadata": {
        "id": "0XlMqior3wY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import your model and start modelling\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.linear_model import SGDRegressor\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model=RandomForestRegressor()\n",
        "model.fit(X_train,Y_train)\n",
        "# model.score(features,labels)\n",
        "# model.coef_\n",
        "# model.intercept_\n",
        "Y_predict=model.predict(X_test)\n",
        "\n",
        "# calulating root mean squared error\n",
        "rmse=np.sqrt(metrics.mean_squared_error(Y_test,Y_predict))\n",
        "print(\"Rmse = \",rmse)\n",
        "\n",
        "#Calculating K-fold cross validation (to check if our model got overfitted)\n",
        "rmse_cross_val_scores=np.sqrt(-1*cross_val_score(model,X_train,Y_train,scoring=\"neg_mean_squared_error\",cv=10))\n",
        "print(\"Rmse cross val scores = \\n\",rmse_cross_val_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZeV19sr1YiR",
        "outputId": "c8e01cad-2781-489a-c34c-07bb0ddbdd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rmse =  3.3789639901985953\n",
            "Rmse cross val scores = \n",
            " [2.87667815 2.90645052 4.44626112 2.62887072 3.43234611 2.54121222\n",
            " 4.512502   3.27714691 3.44440933 3.21893902]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results:-\n",
        "**for LinearRegression**\n",
        "\n",
        "Rmse =  4.22823975454173\n",
        "\n",
        "Rmse cross val scores = \n",
        " [4.21674442 4.26026816 5.1071608  3.82881892 5.34093789 4.3785611\n",
        " 7.47384779 5.48226252 4.14885722 6.0669122 ]\n",
        "\n",
        "**for DecisionTreeRegressor**\n",
        "\n",
        "Rmse =  4.579825752315106\n",
        "\n",
        "Rmse cross val scores =\n",
        " [4.00051826 3.99264568 5.5571751  4.12458424 4.09615674 2.79986607\n",
        " 5.19155564 3.746465   3.25326759 4.28893926]\n",
        "\n",
        " **for SGDRegressor**\n",
        " \n",
        " Rmse =  4.252223739738116\n",
        " \n",
        "Rmse cross val scores = \n",
        " [4.24276292 4.20132664 5.16754377 3.81191694 5.30153767 4.41060904\n",
        " 7.56190827 5.47565146 4.16871895 6.12701213]\n",
        "\n",
        " **for KNeighborsRegressor**\n",
        " \n",
        " Rmse =  4.312169282573288\n",
        " \n",
        "Rmse cross val scores = \n",
        " [3.61967119 4.70763158 5.16523463 4.45849805 4.6198777  3.46735923\n",
        " 8.47209655 5.52478235 3.22961608 4.1353174 ]\n",
        "\n",
        "**for RandomForestRegressor**\n",
        "\n",
        "Rmse =  3.3789639901985953\n",
        "\n",
        "Rmse cross val scores = \n",
        " [2.87667815 2.90645052 4.44626112 2.62887072 3.43234611 2.54121222\n",
        " 4.512502   3.27714691 3.44440933 3.21893902]\n",
        "\n",
        " **Here we can clearly see that our *RandomForestRegressor* model Wins here(as it is giving lowest rmse)**  \n",
        "\n",
        " üôÇüôÇ\n",
        "\n",
        " (Any edits/suggestions are most welcome) üôè"
      ],
      "metadata": {
        "id": "9_GNnzcfI5Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***¬©Abhijit Kumar (NIT Duragapur CSE)***"
      ],
      "metadata": {
        "id": "tnhUHFirKm0t"
      }
    }
  ]
}